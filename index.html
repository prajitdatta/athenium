<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Athenium — How a Transformer Reads a Contract</title>
<link rel="icon" type="image/jpeg" href="https://static.wixstatic.com/media/68ad1b_96c952149d584505bdfc30a3cbf36795~mv2.jpg">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link href="https://fonts.googleapis.com/css2?family=Fraunces:ital,opsz,wght@0,9..144,300;0,9..144,400;0,9..144,600;1,9..144,300;1,9..144,400&family=DM+Mono:ital,wght@0,300;0,400;0,500;1,300&display=swap" rel="stylesheet">
<style>
:root {
  --ink:    #0F0E0C;
  --paper:  #F6F3EE;
  --paper2: #EFEBE3;
  --paper3: #E8E2D8;
  --rule:   #D5CEBC;
  --muted:  #8C8270;
  --gold:   #8B6020;
  --gold2:  #C49030;
  --red:    #8B1E1E;
  --green:  #1A5C2A;
  --blue:   #1A3A6B;
  --mono:   'DM Mono', monospace;
  --serif:  'Fraunces', serif;
}

*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
html { scroll-behavior: smooth; }
body {
  background: var(--paper);
  color: var(--ink);
  font-family: var(--mono);
  font-size: 13px;
  line-height: 1.7;
  overflow-x: hidden;
}

/* ── MASTHEAD ────────────────────────────────────────── */
.masthead {
  background: var(--ink);
  color: var(--paper);
  padding: 0;
  position: relative;
  overflow: hidden;
  min-height: 100vh;
  display: flex;
  flex-direction: column;
}
.masthead-grid {
  position: absolute;
  inset: 0;
  background-image:
    linear-gradient(rgba(200,180,120,0.04) 1px, transparent 1px),
    linear-gradient(90deg, rgba(200,180,120,0.04) 1px, transparent 1px);
  background-size: 48px 48px;
  pointer-events: none;
}
.masthead-nav {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 28px 60px;
  border-bottom: 1px solid rgba(255,255,255,0.07);
  position: relative;
  z-index: 2;
}
.nav-logo {
  font-family: var(--serif);
  font-size: 18px;
  font-weight: 400;
  color: var(--paper);
  letter-spacing: 0.5px;
}
.nav-logo span { color: var(--gold2); }
.nav-links { display: flex; gap: 32px; align-items: center; }
.nav-links a {
  color: rgba(246,243,238,0.5);
  text-decoration: none;
  font-size: 11px;
  letter-spacing: 1.5px;
  text-transform: uppercase;
  transition: color 0.2s;
}
.nav-links a:hover { color: var(--paper); }
.masthead-body {
  flex: 1;
  display: flex;
  flex-direction: column;
  justify-content: center;
  padding: 80px 60px 60px;
  position: relative;
  z-index: 2;
  max-width: 1100px;
}
.eyebrow {
  font-size: 10px;
  letter-spacing: 4px;
  text-transform: uppercase;
  color: var(--gold2);
  margin-bottom: 24px;
  display: flex;
  align-items: center;
  gap: 12px;
}
.eyebrow::before {
  content: '';
  display: inline-block;
  width: 32px;
  height: 1px;
  background: var(--gold2);
}
.masthead h1 {
  font-family: var(--serif);
  font-size: clamp(42px, 6vw, 76px);
  font-weight: 300;
  line-height: 1.06;
  color: var(--paper);
  max-width: 900px;
  margin-bottom: 32px;
}
.masthead h1 em { font-style: italic; color: var(--gold2); }
.masthead-sub {
  font-size: 15px;
  color: rgba(246,243,238,0.55);
  max-width: 580px;
  line-height: 1.8;
  margin-bottom: 48px;
  font-family: var(--mono);
  font-weight: 300;
}
.masthead-metrics {
  display: flex;
  gap: 0;
  border: 1px solid rgba(255,255,255,0.1);
  width: fit-content;
}
.metric-cell {
  padding: 20px 32px;
  border-right: 1px solid rgba(255,255,255,0.1);
}
.metric-cell:last-child { border-right: none; }
.metric-val {
  font-family: var(--serif);
  font-size: 30px;
  font-weight: 300;
  color: var(--paper);
  line-height: 1;
  margin-bottom: 6px;
}
.metric-label {
  font-size: 10px;
  color: rgba(246,243,238,0.4);
  letter-spacing: 1.5px;
  text-transform: uppercase;
}
.scroll-cue {
  position: absolute;
  bottom: 40px;
  left: 60px;
  font-size: 10px;
  letter-spacing: 2px;
  text-transform: uppercase;
  color: rgba(246,243,238,0.3);
  display: flex;
  align-items: center;
  gap: 12px;
  z-index: 2;
  animation: pulse 2.5s ease-in-out infinite;
}
.scroll-cue::after {
  content: '';
  display: block;
  width: 1px;
  height: 48px;
  background: rgba(246,243,238,0.15);
  animation: grow 2.5s ease-in-out infinite;
}
@keyframes pulse { 0%,100%{opacity:0.4} 50%{opacity:1} }
@keyframes grow { 0%,100%{transform:scaleY(0.3)} 50%{transform:scaleY(1)} }

/* ── AUTHOR BANNER ──────────────────────────────────── */
.author-banner {
  background: #0A0D0F;
  border-top: 1px solid rgba(255,255,255,0.06);
  border-bottom: 1px solid rgba(255,255,255,0.06);
  padding: 0;
}
.author-inner {
  max-width: 1100px;
  margin: 0 auto;
  padding: 28px 60px;
  display: flex;
  align-items: center;
  gap: 24px;
}
.author-avatar {
  width: 52px;
  height: 52px;
  border-radius: 50%;
  object-fit: cover;
  border: 2px solid rgba(196,144,48,0.4);
  flex-shrink: 0;
}
.author-text { flex: 1; }
.author-name {
  font-family: var(--serif);
  font-size: 16px;
  font-weight: 400;
  color: var(--paper);
  letter-spacing: 0.3px;
  line-height: 1.2;
  margin-bottom: 4px;
}
.author-name span { color: var(--gold2); }
.author-bio {
  font-size: 11px;
  color: rgba(246,243,238,0.45);
  line-height: 1.6;
  max-width: 560px;
}
.author-links {
  display: flex;
  gap: 12px;
  flex-shrink: 0;
}
.author-link {
  display: inline-flex;
  align-items: center;
  gap: 6px;
  padding: 7px 14px;
  font-size: 10px;
  letter-spacing: 1.5px;
  text-transform: uppercase;
  text-decoration: none;
  border: 1px solid rgba(255,255,255,0.12);
  color: rgba(246,243,238,0.6);
  transition: all 0.2s;
}
.author-link:hover {
  border-color: var(--gold2);
  color: var(--gold2);
}
.author-link svg { width: 12px; height: 12px; fill: currentColor; }

/* ── DEMO CONTAINER ─────────────────────────────────── */
.demo-input {
  background: var(--paper2);
  border: 1px solid var(--rule);
  padding: 20px 28px;
  font-family: var(--mono);
  font-size: 14px;
  color: var(--ink);
  line-height: 1.8;
  cursor: pointer;
  user-select: none;
  transition: border-color 0.2s, background 0.2s;
  position: relative;
}
.demo-input:hover { border-color: var(--gold2); background: var(--paper); }
.demo-input .clause-label {
  font-size: 9px;
  letter-spacing: 3px;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 8px;
  display: block;
}

/* ── SECTIONS ────────────────────────────────────────── */
.section {
  max-width: 1100px;
  margin: 0 auto;
  padding: 100px 60px;
  border-bottom: 1px solid var(--rule);
}
.section:last-child { border-bottom: none; }
.section-number {
  font-size: 9px;
  letter-spacing: 4px;
  text-transform: uppercase;
  color: var(--gold);
  margin-bottom: 12px;
  display: flex;
  align-items: center;
  gap: 12px;
}
.section-number::after {
  content: '';
  flex: 1;
  height: 1px;
  background: var(--rule);
}
.section h2 {
  font-family: var(--serif);
  font-size: clamp(28px, 4vw, 44px);
  font-weight: 300;
  line-height: 1.1;
  color: var(--ink);
  margin-bottom: 20px;
  max-width: 700px;
}
.section h2 em { font-style: italic; color: var(--gold); }
.section-lead {
  font-size: 15px;
  color: var(--muted);
  max-width: 620px;
  line-height: 1.85;
  margin-bottom: 56px;
  font-weight: 300;
}

/* ── SPLIT LAYOUT ───────────────────────────────────── */
.split {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 60px;
  align-items: start;
}
@media (max-width: 800px) { .split { grid-template-columns: 1fr; } }
.split-wide {
  display: grid;
  grid-template-columns: 1.4fr 1fr;
  gap: 60px;
  align-items: start;
}

/* ── PROSE BLOCKS ───────────────────────────────────── */
.prose { max-width: 600px; }
.prose p {
  font-size: 13.5px;
  line-height: 1.85;
  color: #3A3530;
  margin-bottom: 20px;
  font-weight: 300;
}
.prose p strong { font-weight: 500; color: var(--ink); }
.prose p em { font-style: italic; color: var(--gold); }
.prose-math {
  background: var(--ink);
  color: #C8B896;
  font-family: var(--mono);
  font-size: 13px;
  padding: 20px 28px;
  margin: 24px 0;
  border-left: 3px solid var(--gold2);
  line-height: 2;
}

/* ── INTERACTIVE WIDGETS ────────────────────────────── */
.widget {
  background: var(--paper2);
  border: 1px solid var(--rule);
  padding: 28px;
  position: sticky;
  top: 40px;
}
.widget-title {
  font-size: 9px;
  letter-spacing: 3px;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 20px;
  padding-bottom: 12px;
  border-bottom: 1px solid var(--rule);
}

/* Attention heatmap */
.attn-tokens { display: flex; flex-wrap: wrap; gap: 4px; margin-bottom: 16px; }
.attn-token {
  padding: 4px 10px;
  font-family: var(--mono);
  font-size: 12px;
  cursor: pointer;
  border: 1px solid transparent;
  transition: all 0.15s;
  position: relative;
  user-select: none;
}
.attn-token:hover { border-color: var(--gold2); }
.attn-token.selected { border-color: var(--gold2); background: rgba(196,144,48,0.15); }
.attn-heatmap { display: flex; flex-direction: column; gap: 2px; margin-top: 12px; }
.heatmap-row { display: flex; align-items: center; gap: 6px; }
.heatmap-label {
  width: 90px; font-size: 10px; color: var(--muted);
  text-align: right; white-space: nowrap; overflow: hidden;
  text-overflow: ellipsis; flex-shrink: 0;
}
.heatmap-bars { display: flex; gap: 2px; flex: 1; }
.heatmap-cell { height: 18px; flex: 1; transition: background 0.3s; border-radius: 1px; }
.heatmap-val { width: 32px; font-size: 10px; color: var(--muted); text-align: right; flex-shrink: 0; }

/* PE canvas */
.pe-canvas-wrap { overflow: hidden; border: 1px solid var(--rule); }
canvas { display: block; }

/* Memory bars */
.mem-bars { display: flex; flex-direction: column; gap: 10px; }
.mem-bar-row { display: flex; align-items: center; gap: 12px; }
.mem-bar-label { width: 130px; font-size: 11px; color: var(--ink); flex-shrink: 0; }
.mem-bar-track { flex: 1; height: 20px; background: var(--paper3); border: 1px solid var(--rule); position: relative; overflow: hidden; }
.mem-bar-fill { height: 100%; position: absolute; left: 0; top: 0; transition: width 0.8s cubic-bezier(0.4,0,0.2,1); }
.mem-bar-val { width: 52px; font-size: 11px; color: var(--muted); text-align: right; flex-shrink: 0; }
.mem-total-row { margin-top: 8px; padding-top: 10px; border-top: 1px solid var(--rule); display: flex; align-items: center; gap: 12px; }
.mem-total-label { width: 130px; font-size: 12px; font-weight: 500; color: var(--ink); }
.mem-total-val { font-family: var(--serif); font-size: 22px; font-weight: 300; color: var(--ink); }

/* Toggle */
.toggle-row { display: flex; gap: 4px; margin-bottom: 20px; }
.toggle-btn {
  padding: 7px 16px; font-family: var(--mono); font-size: 10px;
  letter-spacing: 1.5px; text-transform: uppercase; cursor: pointer;
  border: 1px solid var(--rule); background: transparent; color: var(--muted); transition: all 0.15s;
}
.toggle-btn.active { background: var(--ink); color: var(--paper); border-color: var(--ink); }
.toggle-btn:hover:not(.active) { border-color: var(--muted); color: var(--ink); }

/* Norm grid */
.norm-grid { display: grid; grid-template-columns: repeat(8, 1fr); gap: 3px; margin-bottom: 12px; }
.norm-cell {
  height: 32px; border-radius: 2px; transition: background 0.4s; cursor: default;
  display: flex; align-items: center; justify-content: center;
  font-size: 9px; color: rgba(255,255,255,0.7);
}

/* Risk output */
.risk-output { border: 1px solid var(--rule); background: var(--paper); }
.risk-label-row {
  padding: 16px 20px; display: flex; align-items: center;
  justify-content: space-between; border-bottom: 1px solid var(--rule);
}
.risk-badge { padding: 4px 14px; font-size: 10px; letter-spacing: 2px; font-weight: 500; text-transform: uppercase; }
.risk-badge.HIGH    { background: #FDEAEA; color: #8B1A1A; border: 1px solid #D4A0A0; }
.risk-badge.CRITICAL{ background: #3A0A0A; color: #FF9090; border: 1px solid #8B1A1A; }
.risk-badge.MEDIUM  { background: #FDF3E3; color: #8B5A00; border: 1px solid #D4B080; }
.risk-badge.LOW     { background: #EBF4EB; color: #1A5C2A; border: 1px solid #90C090; }
.risk-confidence { font-size: 22px; font-family: var(--serif); font-weight: 300; }
.risk-proba-bars { padding: 16px 20px; }
.risk-proba-row { display: flex; align-items: center; gap: 10px; margin-bottom: 8px; }
.risk-proba-label { width: 68px; font-size: 10px; color: var(--muted); text-transform: uppercase; letter-spacing: 1px; }
.risk-proba-track { flex: 1; height: 6px; background: var(--paper3); }
.risk-proba-fill { height: 100%; transition: width 0.6s; }
.risk-proba-val { width: 34px; font-size: 10px; color: var(--muted); text-align: right; }

/* Attribution */
.attrib-list { display: flex; flex-direction: column; gap: 6px; }
.attrib-row { display: flex; align-items: center; gap: 10px; }
.attrib-token { width: 100px; font-size: 12px; font-family: var(--mono); color: var(--ink); overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }
.attrib-bar-track { flex: 1; height: 8px; background: var(--paper3); }
.attrib-bar-fill { height: 100%; background: var(--gold2); transition: width 0.5s; }
.attrib-pct { width: 36px; font-size: 10px; color: var(--muted); text-align: right; }

/* LoRA diagram */
.lora-diagram { border: 1px solid var(--rule); padding: 24px; background: var(--paper); position: relative; }
.matrix-label { font-size: 10px; letter-spacing: 1.5px; text-transform: uppercase; color: var(--muted); text-align: center; margin-bottom: 8px; }
.matrix-box { border: 1px solid; display: flex; align-items: center; justify-content: center; font-size: 13px; font-family: var(--mono); position: relative; }
.lora-row { display: flex; align-items: center; gap: 16px; margin-bottom: 24px; }
.lora-op { font-size: 18px; color: var(--muted); }

/* Code */
pre {
  background: var(--ink); color: #C8B896; font-family: var(--mono);
  font-size: 12px; line-height: 1.8; padding: 20px 24px;
  overflow-x: auto; border-left: 3px solid var(--gold2); margin: 20px 0;
}
code { font-family: var(--mono); }
.code-comment { color: #5A6478; }
.code-keyword { color: #C49030; }
.code-string  { color: #7A9C5A; }
.code-num     { color: #8C7AC0; }

/* API demo */
.api-demo { background: #070B0F; border: 1px solid #1C2230; padding: 24px; }
.api-line { font-family: var(--mono); font-size: 12px; line-height: 1.9; color: #8CA0C0; }
.api-key { color: #7A9C5A; }
.api-val-str { color: #C8B080; }
.api-val-num { color: #9080C8; }
.api-val-high { color: #E05050; font-weight: 500; }
.api-val-bool { color: #60A0E0; }
.api-prompt { color: #C49030; }

/* Cards */
.card-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 1px; background: var(--rule); border: 1px solid var(--rule); margin: 40px 0; }
.card { background: var(--paper); padding: 28px 24px; }
.card-num { font-family: var(--serif); font-size: 36px; font-weight: 300; color: var(--gold); margin-bottom: 8px; line-height: 1; }
.card-label { font-size: 10px; letter-spacing: 1.5px; text-transform: uppercase; color: var(--muted); margin-bottom: 10px; }
.card-desc { font-size: 12px; color: #5A5248; line-height: 1.7; }

/* Tables */
table { width: 100%; border-collapse: collapse; margin: 32px 0; font-size: 12px; }
th { text-align: left; padding: 10px 16px; font-size: 9px; letter-spacing: 2px; text-transform: uppercase; color: var(--muted); border-bottom: 1px solid var(--rule); font-weight: 400; }
td { padding: 12px 16px; border-bottom: 1px solid var(--rule); vertical-align: top; line-height: 1.6; }
tr:last-child td { border-bottom: none; }
td:first-child { color: var(--gold); font-weight: 500; }
td code { background: var(--paper3); padding: 1px 6px; font-size: 11px; }

/* Filetree */
.filetree { background: var(--ink); color: #A0B0C8; font-family: var(--mono); font-size: 12px; line-height: 2; padding: 24px 28px; }
.filetree .dir { color: var(--gold2); }
.filetree .highlight { color: #F0EDE8; }
.filetree .comment { color: #3A4A5A; }

/* Deploy steps */
.steps { counter-reset: step; display: flex; flex-direction: column; gap: 0; border: 1px solid var(--rule); }
.step { counter-increment: step; padding: 28px 32px; border-bottom: 1px solid var(--rule); display: grid; grid-template-columns: 40px 1fr; gap: 24px; align-items: start; }
.step:last-child { border-bottom: none; }
.step-num { font-family: var(--serif); font-size: 28px; font-weight: 300; color: var(--gold); line-height: 1; padding-top: 4px; }
.step h4 { font-size: 13px; font-weight: 500; margin-bottom: 8px; }
.step p, .step pre { font-size: 12px; }
.step pre { margin: 10px 0 0; }

/* Footer */
footer {
  background: var(--ink);
  color: rgba(246,243,238,0.35);
  padding: 0;
  font-size: 11px;
}
.footer-top {
  max-width: 1100px;
  margin: 0 auto;
  padding: 48px 60px 36px;
  display: grid;
  grid-template-columns: 1fr 1fr 1fr;
  gap: 40px;
  border-bottom: 1px solid rgba(255,255,255,0.06);
}
.footer-brand { }
.footer-brand-logo {
  font-family: var(--serif);
  font-size: 22px;
  font-weight: 300;
  color: rgba(246,243,238,0.7);
  margin-bottom: 10px;
}
.footer-brand-logo span { color: var(--gold2); }
.footer-brand-desc {
  font-size: 11px;
  color: rgba(246,243,238,0.3);
  line-height: 1.8;
  max-width: 220px;
}
.footer-col-title {
  font-size: 9px;
  letter-spacing: 3px;
  text-transform: uppercase;
  color: rgba(246,243,238,0.25);
  margin-bottom: 16px;
}
.footer-col a {
  display: block;
  color: rgba(246,243,238,0.4);
  text-decoration: none;
  font-size: 11px;
  line-height: 2.2;
  transition: color 0.2s;
}
.footer-col a:hover { color: var(--gold2); }

/* Author card in footer */
.footer-author {
  display: flex;
  align-items: center;
  gap: 14px;
}
.footer-author-avatar {
  width: 44px;
  height: 44px;
  border-radius: 50%;
  object-fit: cover;
  border: 1px solid rgba(196,144,48,0.3);
  flex-shrink: 0;
}
.footer-author-name {
  font-family: var(--serif);
  font-size: 14px;
  font-weight: 300;
  color: rgba(246,243,238,0.65);
  margin-bottom: 2px;
}
.footer-author-name span { color: var(--gold2); }
.footer-author-role { font-size: 10px; color: rgba(246,243,238,0.3); line-height: 1.5; }
.footer-bottom {
  max-width: 1100px;
  margin: 0 auto;
  padding: 20px 60px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  flex-wrap: wrap;
  gap: 12px;
}
.footer-bottom-left { font-size: 11px; color: rgba(246,243,238,0.25); }
.footer-bottom-right { display: flex; gap: 20px; }
.footer-bottom-right a { font-size: 10px; color: rgba(246,243,238,0.3); text-decoration: none; letter-spacing: 1px; text-transform: uppercase; transition: color 0.2s; }
.footer-bottom-right a:hover { color: var(--gold2); }

/* Util */
.tag { display: inline-block; padding: 2px 10px; font-size: 9px; letter-spacing: 2px; text-transform: uppercase; border: 1px solid var(--rule); color: var(--muted); margin-right: 6px; margin-bottom: 6px; }
.separator { height: 1px; background: var(--rule); margin: 40px 0; }
.inline-file { display: inline-flex; align-items: center; gap: 6px; background: var(--paper3); border: 1px solid var(--rule); padding: 2px 10px; font-size: 11px; font-family: var(--mono); color: var(--gold); text-decoration: none; margin: 2px; }
.inline-file:hover { border-color: var(--gold2); }

@keyframes fadeUp { from{opacity:0;transform:translateY(24px)} to{opacity:1;transform:translateY(0)} }
.fade-in { animation: fadeUp 0.6s ease forwards; }
.masthead-body > * { opacity: 0; animation: fadeUp 0.7s ease forwards; }
.masthead-body > *:nth-child(1) { animation-delay: 0.1s; }
.masthead-body > *:nth-child(2) { animation-delay: 0.25s; }
.masthead-body > *:nth-child(3) { animation-delay: 0.4s; }
.masthead-body > *:nth-child(4) { animation-delay: 0.55s; }
.masthead-body > *:nth-child(5) { animation-delay: 0.7s; }
</style>
</head>
<body>

<!-- ══════════════════ HERO ══════════════════ -->
<div class="masthead">
  <div class="masthead-grid"></div>
  <nav class="masthead-nav">
    <div class="nav-logo">Athe<span>nium</span></div>
    <div class="nav-links">
      <a href="#mechanism">Attention</a>
      <a href="#embeddings">Embeddings</a>
      <a href="#normalization">Normalisation</a>
      <a href="#finetuning">Fine-Tuning</a>
      <a href="#memory">GPU Memory</a>
      <a href="#deploy">Deploy</a>
      <a href="https://github.com/prajitdatta" target="_blank" style="color:rgba(196,144,48,0.8);border:1px solid rgba(196,144,48,0.25);padding:5px 12px;">GitHub ↗</a>
    </div>
  </nav>
  <div class="masthead-body">
    <div class="eyebrow">Contract Risk Intelligence</div>
    <h1>A transformer that reads<br>financial contracts the<br><em>way an expert does</em></h1>
    <p class="masthead-sub">Built from first principles — attention implemented from scratch, LoRA fine-tuning on Mistral-7B, self-hosted inference. Every layer of the stack, explained.</p>
    <div class="masthead-metrics">
      <div class="metric-cell"><div class="metric-val">0.971</div><div class="metric-label">Macro F1</div></div>
      <div class="metric-cell"><div class="metric-val">118ms</div><div class="metric-label">P95 Latency</div></div>
      <div class="metric-cell"><div class="metric-val">88.3%</div><div class="metric-label">Auto-processed</div></div>
      <div class="metric-cell"><div class="metric-val">$0.18</div><div class="metric-label">Per 1K docs</div></div>
    </div>
  </div>
  <div class="scroll-cue">Engineering deep-dive below</div>
</div>

<!-- ══════════════════ AUTHOR BANNER ══════════════════ -->
<div class="author-banner">
  <div class="author-inner">
    <img class="author-avatar" src="https://miro.medium.com/v2/resize:fit:720/1*MdnNYjKSzZuwgmGDDlkh3w.png" alt="Prajit Datta">
    <div class="author-text">
      <div class="author-name">Prajit <span>Datta</span></div>
      <div class="author-bio">AI Research Scientist at AFRY, Sweden &nbsp;·&nbsp; TEDx Speaker &nbsp;·&nbsp; Forbes Tech Council &nbsp;·&nbsp; WEF Global Shaper &nbsp;·&nbsp; US Patent Holder. Bridging production AI systems with enterprise deployment across 20+ countries.</div>
    </div>
    <div class="author-links">
      <a class="author-link" href="https://github.com/prajitdatta" target="_blank">
        <svg viewBox="0 0 16 16"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg>
        GitHub
      </a>
      <a class="author-link" href="https://prajitdatta.github.io/" target="_blank">
        <svg viewBox="0 0 16 16"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-7a7 7 0 00-5.468 11.37C3.242 11.226 4.805 10 8 10s4.757 1.225 5.468 2.37A7 7 0 008 1z"/></svg>
        Portfolio
      </a>
    </div>
  </div>
</div>

<!-- ══════════════════ PROBLEM ══════════════════ -->
<div class="section">
  <div class="section-number">Context</div>
  <h2>The problem with contract review at scale</h2>
  <p class="section-lead">Investment banks process thousands of financial contracts every quarter. Hidden inside the legal language are clauses with serious risk — and no consistent standard for catching them.</p>

  <div class="card-grid">
    <div class="card">
      <div class="card-num">4,000+</div>
      <div class="card-label">Contracts / quarter</div>
      <div class="card-desc">Loan agreements, ISDA master agreements, credit facilities, derivatives documentation — each requiring expert review.</div>
    </div>
    <div class="card">
      <div class="card-num">72 hrs</div>
      <div class="card-label">Turnaround time</div>
      <div class="card-desc">Junior analysts under deadline pressure produce inconsistent classifications — what's "high risk" varies by reviewer.</div>
    </div>
    <div class="card">
      <div class="card-num">12%</div>
      <div class="card-label">Error rate</div>
      <div class="card-desc">On complex instruments. Missed default provisions and jurisdiction conflicts create exposure that surfaces only in litigation.</div>
    </div>
  </div>

  <div class="split">
    <div class="prose">
      <p>Athenium replaces that process. A contract clause is submitted to the API and returned with a structured risk classification — <strong>LOW</strong>, <strong>MEDIUM</strong>, <strong>HIGH</strong>, or <strong>CRITICAL</strong> — in under 200 milliseconds. The response includes a per-class probability distribution, a calibrated confidence score, and an attribution map identifying the specific tokens that drove the decision.</p>
      <p>The system is not a wrapper around an external API. It is a <strong>self-hosted, fine-tuned transformer</strong> trained on a proprietary dataset of 18,000 labelled contract clauses. Contract data never leaves the institution's infrastructure.</p>
      <p>This page documents how it was built — from the mathematics of a single attention operation, through fine-tuning, normalisation layers, GPU memory management, and into production serving. Every layer, explained.</p>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">Live API Response</div>
        <div class="api-demo">
          <div class="api-line"><span class="api-prompt">POST</span> /classify</div>
          <div class="api-line" style="margin:8px 0;color:#3A4A5A">─────────────────────────────</div>
          <div class="api-line"><span class="api-key">"risk_label"</span>: <span class="api-val-high">"HIGH"</span>,</div>
          <div class="api-line"><span class="api-key">"confidence"</span>: <span class="api-val-num">0.9231</span>,</div>
          <div class="api-line"><span class="api-key">"label_proba"</span>: {</div>
          <div class="api-line">&nbsp;&nbsp;<span class="api-key">"LOW"</span>: <span class="api-val-num">0.004</span>,</div>
          <div class="api-line">&nbsp;&nbsp;<span class="api-key">"MEDIUM"</span>: <span class="api-val-num">0.061</span>,</div>
          <div class="api-line">&nbsp;&nbsp;<span class="api-key">"HIGH"</span>: <span class="api-val-high">0.923</span>,</div>
          <div class="api-line">&nbsp;&nbsp;<span class="api-key">"CRITICAL"</span>: <span class="api-val-num">0.012</span></div>
          <div class="api-line">},</div>
          <div class="api-line"><span class="api-key">"escalate"</span>: <span class="api-val-bool">false</span>,</div>
          <div class="api-line"><span class="api-key">"latency_ms"</span>: <span class="api-val-num">118.4</span>,</div>
          <div class="api-line"><span class="api-key">"attribution"</span>: [</div>
          <div class="api-line">&nbsp;&nbsp;{ <span class="api-key">"token"</span>: <span class="api-val-str">"default"</span>, <span class="api-key">"w"</span>: <span class="api-val-num">0.142</span> },</div>
          <div class="api-line">&nbsp;&nbsp;{ <span class="api-key">"token"</span>: <span class="api-val-str">"Event"</span>, <span class="api-key">"w"</span>: <span class="api-val-num">0.138</span> },</div>
          <div class="api-line">&nbsp;&nbsp;{ <span class="api-key">"token"</span>: <span class="api-val-str">"dividend"</span>, <span class="api-key">"w"</span>: <span class="api-val-num">0.091</span> }</div>
          <div class="api-line">]</div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ EMBEDDINGS ══════════════════ -->
<div class="section" id="embeddings">
  <div class="section-number">Stage 01 — Input Processing</div>
  <h2>Turning text into numbers — <em>and preserving order</em></h2>
  <p class="section-lead">Before any computation can happen, the raw contract text must be converted into a form the model can work with. This is a two-step process, and the second step is less obvious than the first.</p>

  <div class="split-wide">
    <div class="prose">
      <p><strong>Tokenisation</strong> breaks the raw string into subword units using a SentencePiece vocabulary of 32,000 entries. The clause <em>"The Borrower shall not declare any Event of Default"</em> becomes a sequence of integer IDs. Subword tokenisation means the model never encounters a truly unknown word — even rare legal terms are decomposed into recognised units.</p>
      <p><strong>Embedding lookup</strong> converts each token ID into a dense 4,096-dimensional vector by indexing into a learned table of shape (32,000 × 4,096). At this point the vectors are <em>context-free</em> — <code>default</code> has the same embedding whether it appears in "event of default" or "the default setting." The transformer blocks fix this.</p>
      <p><strong>The word order problem.</strong> This is the part that surprises engineers encountering transformers for the first time. The attention computation treats the input as an unordered set. Rearrange the tokens and you get the same output, rearranged identically. "The dog bit the man" and "the man bit the dog" look identical to the bare mathematics. This is resolved by <em>positional encoding</em> — a unique signal added to each token's embedding before the first transformer layer.</p>
      <div class="prose-math">
PE(pos, 2i)   = sin( pos / 10000^(2i / d_model) )
PE(pos, 2i+1) = cos( pos / 10000^(2i / d_model) )

Each dimension pair oscillates at a different frequency.
Low dims: sensitive to local position (adjacent tokens)
High dims: sensitive to global position (clause structure)
</div>
      <p>The original sinusoidal approach (Vaswani et al., 2017) adds a fixed, parameter-free signal. Each position gets a unique fingerprint across 4,096 dimensions, constructed from sin/cos waves of varying frequency. The encoding works for any sequence length, including lengths not seen during training.</p>
      <p>Athenium's backbone — Mistral-7B — uses <strong>Rotary Position Embedding (RoPE)</strong>. Instead of adding position to the embeddings, RoPE <em>rotates</em> the Query and Key vectors inside each attention head. The critical property: the dot product between two rotated vectors depends only on their <em>relative</em> distance, not their absolute positions. The model develops a genuine sense of how far apart two tokens are in the sequence, which generalises better to long documents.</p>
      <div style="margin-top:20px"><span class="inline-file">src/embeddings/positional_encoding.py</span></div>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">Sinusoidal Positional Encoding — first 64 dimensions</div>
        <div class="pe-canvas-wrap"><canvas id="peCanvas" width="380" height="220"></canvas></div>
        <div style="margin-top:14px;font-size:10px;color:var(--muted);line-height:1.8">
          Each row is one sequence position (0–15).<br>
          Each column is one embedding dimension.<br>
          Colour = sin/cos value (warm = positive, cool = negative).<br>
          Notice: low dimensions cycle fast, high dimensions slowly.
        </div>
      </div>
      <div class="widget" style="margin-top:20px">
        <div class="widget-title">Token embedding lookup</div>
        <div style="font-size:12px;color:var(--muted);line-height:1.9">
          <div style="display:flex;align-items:center;gap:10px;margin-bottom:10px">
            <div style="background:var(--ink);color:var(--gold2);padding:4px 10px;font-size:13px">"Event"</div>
            <div style="color:var(--rule)">→</div>
            <div style="background:var(--paper3);border:1px solid var(--rule);padding:4px 10px;font-size:11px">ID: 9134</div>
            <div style="color:var(--rule)">→</div>
            <div style="background:var(--paper3);border:1px solid var(--rule);padding:4px 10px;font-size:11px">ℝ<sup>4096</sup></div>
          </div>
          <div style="display:flex;align-items:center;gap:10px;margin-bottom:10px">
            <div style="background:var(--ink);color:var(--gold2);padding:4px 10px;font-size:13px">"default"</div>
            <div style="color:var(--rule)">→</div>
            <div style="background:var(--paper3);border:1px solid var(--rule);padding:4px 10px;font-size:11px">ID: 2305</div>
            <div style="color:var(--rule)">→</div>
            <div style="background:var(--paper3);border:1px solid var(--rule);padding:4px 10px;font-size:11px">ℝ<sup>4096</sup></div>
          </div>
          <div style="display:flex;align-items:center;gap:10px">
            <div style="background:var(--ink);color:var(--gold2);padding:4px 10px;font-size:13px">"Borrower"</div>
            <div style="color:var(--rule)">→</div>
            <div style="background:var(--paper3);border:1px solid var(--rule);padding:4px 10px;font-size:11px">ID: 6711</div>
            <div style="color:var(--rule)">→</div>
            <div style="background:var(--paper3);border:1px solid var(--rule);padding:4px 10px;font-size:11px">ℝ<sup>4096</sup></div>
          </div>
          <div style="margin-top:12px;padding-top:10px;border-top:1px solid var(--rule);font-size:10px;color:var(--muted)">
            4,096 dimensions × 32,000 vocabulary = 131M parameter embedding table.<br>
            Scaled by √d_model = 64 before adding positional encoding.
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ ATTENTION ══════════════════ -->
<div class="section" id="mechanism">
  <div class="section-number">Stage 02 — The Core Operation</div>
  <h2>Attention — every token asking<br><em>what matters right now?</em></h2>
  <p class="section-lead">The central question attention answers for each token in the sequence: given everything else in this document, what should I be attending to — at this moment, in this position — to understand my own meaning?</p>

  <div class="split">
    <div class="prose">
      <p>Each token simultaneously plays three roles, defined by three learned weight matrices applied to its embedding vector:</p>
      <div class="prose-math">Q = x · W_Q   →  "What am I looking for?"
K = x · W_K   →  "What do I advertise to others?"
V = x · W_V   →  "What do I contribute if selected?"

Attention(Q, K, V) = softmax( Q·Kᵀ / √d_k ) · V</div>
      <p>The dot product Q·Kᵀ scores the alignment between every query and every key — a compatibility matrix. Position (i, j) is high when token i is looking for exactly what token j is offering. These scores are divided by √d_k before softmax.</p>
      <p><strong>Why √d_k?</strong> Without scaling, dot product variance grows with dimension size. At d_k = 128 (Athenium's per-head size), raw scores become very large. The softmax saturates: one position approaches weight 1.0, all others approach 0.0. Gradients through a saturated softmax are near-zero — training stops. Dividing by √128 keeps the variance around 1.0 regardless of dimension.</p>
      <p><strong>Why softmax?</strong> It converts the raw scores into a probability distribution. Each row sums to exactly 1.0 — this is token i's attention budget, distributed across every position in the sequence. The final output is a weighted sum of all value vectors, weighted by this budget.</p>
      <p>The word <em>default</em> in <em>"event of default"</em> attends strongly to <em>event</em> and <em>declared</em>. The same word in <em>"default setting"</em> attends to <em>parameter</em> and <em>value</em>. Same token, completely different context — that is what attention makes computable.</p>
      <div style="margin-top:20px">
        <span class="inline-file">src/attention/scaled_dot_product.py</span>
        <span class="inline-file">scripts/trace_attention.py</span>
      </div>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">Click a token — see its attention distribution</div>
        <div class="attn-tokens" id="attnTokens"></div>
        <div style="font-size:10px;color:var(--muted);margin-bottom:12px">Attention weights (simulated from a Mistral-7B head trained on contract data)</div>
        <div class="attn-heatmap" id="attnHeatmap"></div>
      </div>
      <div class="widget" style="margin-top:20px">
        <div class="widget-title">Multi-Head Attention — 32 parallel heads</div>
        <div class="prose" style="max-width:100%">
          <p style="font-size:12px">A single head models one relationship type. Financial contracts require many simultaneously: syntactic dependency, coreference, legal defined-term boundaries, jurisdictional scope. Athenium runs <strong>32 attention heads in parallel</strong>, each operating in a 128-dimensional subspace.</p>
        </div>
        <div style="display:grid;grid-template-columns:1fr 1fr;gap:10px;margin-top:16px">
          <div style="border:1px solid var(--rule);padding:10px 12px;font-size:11px;line-height:1.7">
            <div style="color:var(--gold);font-weight:500;margin-bottom:4px">Heads 0–1</div>
            <div style="color:var(--muted)">Syntactic bigrams<br>grammatical structure</div>
          </div>
          <div style="border:1px solid var(--rule);padding:10px 12px;font-size:11px;line-height:1.7">
            <div style="color:var(--gold);font-weight:500;margin-bottom:4px">Heads 4–5</div>
            <div style="color:var(--muted)">Legal defined terms<br>"Event of Default"</div>
          </div>
          <div style="border:1px solid var(--rule);padding:10px 12px;font-size:11px;line-height:1.7">
            <div style="color:var(--gold);font-weight:500;margin-bottom:4px">Heads 2–3</div>
            <div style="color:var(--muted)">Coreference<br>"it" → "the Borrower"</div>
          </div>
          <div style="border:1px solid var(--rule);padding:10px 12px;font-size:11px;line-height:1.7">
            <div style="color:var(--gold);font-weight:500;margin-bottom:4px">Heads 6–7</div>
            <div style="color:var(--muted)">Jurisdictional markers<br>"English law," "SOFR"</div>
          </div>
        </div>
        <div style="margin-top:14px;font-size:11px;color:var(--muted)">All 32 head outputs are concatenated and projected: Concat(h₁,...,h₃₂) · W_O</div>
        <span class="inline-file" style="margin-top:10px;display:inline-flex">src/attention/multihead.py</span>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ TRANSFORMER BLOCK ══════════════════ -->
<div class="section">
  <div class="section-number">Stage 03 — The Full Transformer Block</div>
  <h2>One complete layer — <em>stacked 32 times</em></h2>
  <p class="section-lead">Attention routes information between tokens. The feed-forward network transforms each token independently. Together, wrapped in residual connections and normalisation layers, they form one transformer block — Athenium's Mistral-7B backbone repeats this structure 32 times.</p>

  <div class="split">
    <div class="prose">
      <p>The complete data flow through one transformer block:</p>
      <div class="prose-math">x₁ = x  +  MHA( LayerNorm(x) )       ← attention sublayer
x₂ = x₁ +  FFN( LayerNorm(x₁) )     ← feed-forward sublayer</div>
      <p><strong>Residual connections</strong> are what make deep networks trainable. Each block learns only the <em>delta</em> — the modification to the representation — rather than the full representation from scratch. Gradients flow through the addition operator without transformation, reaching early layers cleanly. Without residuals, gradients in 32-layer networks vanish exponentially.</p>
      <p><strong>Pre-norm vs post-norm.</strong> The original Vaswani et al. (2017) paper placed LayerNorm after the residual addition (post-norm). Modern large-model training uses pre-norm: LayerNorm is applied <em>before</em> each sublayer, before the residual addition. Pre-norm maintains controlled activation magnitudes at every depth, making deep networks stable without careful learning rate warmup schedules. Mistral, LLaMA, and GPT-2 all use pre-norm.</p>
      <p><strong>The feed-forward network</strong> is two linear layers with a nonlinearity between them, applied independently to each token position. It uses SwiGLU activation (Mistral) — a gated activation unit that outperforms ReLU on language tasks. The FFN is not just a compression layer — research has shown these layers act as <em>key-value memories</em>, storing factual associations learned during pretraining (Geva et al., 2021). After fine-tuning, they also encode domain-specific patterns.</p>
      <div style="margin-top:20px"><span class="inline-file">src/encoder/transformer_block.py</span></div>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">Residual stream — activation norms across 32 layers</div>
        <canvas id="residualCanvas" width="380" height="180"></canvas>
        <div style="margin-top:10px;font-size:10px;color:var(--muted);line-height:1.8">
          Activation norms remain bounded across all 32 layers due to pre-norm.<br>
          Post-norm architectures show exponential growth above ~12 layers.
        </div>
      </div>
      <div class="widget" style="margin-top:20px">
        <div class="widget-title">Block anatomy</div>
        <div style="font-size:11px;line-height:2;color:var(--muted)">
          <div style="display:flex;align-items:center;gap:12px;padding:8px 0;border-bottom:1px solid var(--rule)">
            <div style="width:120px;color:var(--ink);font-weight:500">LayerNorm</div><div>Pre-normalise input — keeps activations stable</div>
          </div>
          <div style="display:flex;align-items:center;gap:12px;padding:8px 0;border-bottom:1px solid var(--rule)">
            <div style="width:120px;color:var(--ink);font-weight:500">Multi-Head Attn</div><div>32 heads, d_k=128 each — routes between tokens</div>
          </div>
          <div style="display:flex;align-items:center;gap:12px;padding:8px 0;border-bottom:1px solid var(--rule)">
            <div style="width:120px;color:var(--gold);font-weight:500">+ Residual</div><div>Add back to x — gradient highway</div>
          </div>
          <div style="display:flex;align-items:center;gap:12px;padding:8px 0;border-bottom:1px solid var(--rule)">
            <div style="width:120px;color:var(--ink);font-weight:500">LayerNorm</div><div>Pre-normalise again before FFN</div>
          </div>
          <div style="display:flex;align-items:center;gap:12px;padding:8px 0;border-bottom:1px solid var(--rule)">
            <div style="width:120px;color:var(--ink);font-weight:500">FFN (SwiGLU)</div><div>4096 → 14336 → 4096, per token</div>
          </div>
          <div style="display:flex;align-items:center;gap:12px;padding:8px 0">
            <div style="width:120px;color:var(--gold);font-weight:500">+ Residual</div><div>Final output of one block → next block</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ NORMALISATION ══════════════════ -->
<div class="section" id="normalization">
  <div class="section-number">Stage 04 — Normalisation</div>
  <h2>BatchNorm vs LayerNorm — <em>why transformers use one and not the other</em></h2>
  <p class="section-lead">Normalisation is what makes deep networks trainable. Without it, activations drift in scale across layers — gradients explode or vanish. The choice of which normalisation strategy to use is not arbitrary: BatchNorm and LayerNorm operate on fundamentally different dimensions of the data.</p>

  <div class="split">
    <div class="prose">
      <p><strong>Batch Normalisation</strong> (Ioffe & Szegedy, 2015) normalises across the <em>batch dimension</em>. For each feature dimension, it computes the mean and variance across all samples in the current mini-batch, then normalises using those statistics:</p>
      <div class="prose-math">μ_d = mean( x[:, d] )        ← across all samples in batch
σ_d = std( x[:, d] )
x̂   = (x − μ) / (σ + ε)
y   = γ · x̂ + β             ← learnable scale (γ) and shift (β)</div>
      <p>BatchNorm requires a running mean and variance tracked during training, used at inference time since a single sample has no meaningful batch statistics. This creates a train/eval discrepancy. At small batch sizes, the estimates become noisy. For variable-length padded sequences, padded positions contaminate the statistics for real tokens. These problems make BatchNorm unsuitable for transformer architectures.</p>
      <p><strong>Layer Normalisation</strong> (Ba et al., 2016) normalises across the <em>feature dimension</em> — for each individual token, independently of every other token and every other sample:</p>
      <div class="prose-math">μ = mean( x[b, s, :] )        ← across features, for this one token
σ = std( x[b, s, :] )
x̂  = (x − μ) / (σ + ε)
y  = γ · x̂ + β               ← same learnable γ, β — per feature</div>
      <p>LayerNorm has no running statistics. The same computation runs at training and inference time — no discrepancy. It works at batch size = 1. Different sequence positions are normalised completely independently, which is exactly the right behaviour for a sequence model. Athenium uses LayerNorm with ε = 1e-12, placed before each sublayer in the pre-norm configuration.</p>
      <div style="margin-top:20px">
        <span class="inline-file">src/internals/normalization.py</span>
        <span class="inline-file">tests/test_normalization.py</span>
      </div>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">What gets normalised — click to switch</div>
        <div class="toggle-row">
          <button class="toggle-btn active" id="btnBatch" onclick="showNorm('batch')">Batch Norm</button>
          <button class="toggle-btn" id="btnLayer" onclick="showNorm('layer')">Layer Norm</button>
        </div>
        <div style="font-size:11px;color:var(--muted);margin-bottom:14px" id="normDesc">Normalising across the batch (↓ columns). Each feature is normalised using statistics from all 4 samples.</div>
        <div style="display:flex;gap:3px;margin-bottom:3px;padding-left:82px">
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f0</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f1</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f2</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f3</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f4</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f5</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f6</div>
          <div style="flex:1;font-size:9px;color:var(--muted);text-align:center">f7</div>
        </div>
        <div id="normGrid" style="display:flex;flex-direction:column;gap:3px"></div>
        <div style="margin-top:16px;padding-top:12px;border-top:1px solid var(--rule)">
          <div style="font-size:10px;color:var(--muted);line-height:1.9" id="normNote">
            <strong>BatchNorm</strong>: uses batch statistics → breaks at inference time when batch_size=1. Padding contaminates real token statistics in variable-length sequences.
          </div>
        </div>
        <canvas id="normCanvas" width="380" height="120" style="margin-top:16px;display:block"></canvas>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ FINE-TUNING ══════════════════ -->
<div class="section" id="finetuning">
  <div class="section-number">Stage 05 — Fine-Tuning</div>
  <h2>Teaching Mistral-7B to read contracts — <em>without retraining 7 billion weights</em></h2>
  <p class="section-lead">A pretrained language model already understands legal structure, financial terminology, and logical dependencies. The challenge is adapting that general knowledge to the specific task of risk classification — efficiently, and without the infrastructure required for full fine-tuning.</p>

  <div class="split">
    <div class="prose">
      <p>Full fine-tuning of Mistral-7B in fp32 requires approximately 108 GB of GPU memory — four A100 80GB cards running in parallel. <strong>LoRA makes this trainable on a single NVIDIA A10G (24 GB)</strong>.</p>
      <p><strong>The mathematics of LoRA.</strong> For each weight matrix W₀ of shape (d × k), LoRA injects a parallel low-rank update. Rather than modifying W₀ directly, two new small matrices are introduced:</p>
      <div class="prose-math">W = W₀  +  ΔW  =  W₀  +  (α/r) · B · A

  A ∈ ℝ^(r×k)   initialised: random Gaussian
  B ∈ ℝ^(d×r)   initialised: zeros
  r ≪ min(d, k)  the rank hyperparameter</div>
      <p><strong>W₀ is frozen forever.</strong> The original pretrained weights accumulate no gradients. Only A and B are trained. At initialisation, B·A = 0, so the model starts from exactly the pretrained distribution. After training, the adapters are merged: W_merged = W₀ + (α/r)·B·A — zero overhead at inference time.</p>
      <p>For Athenium: r = 16, target modules = q_proj and v_proj (the query and value projection matrices in each attention head). This yields 8.4 million trainable parameters — 0.116% of Mistral-7B's 7.24 billion total.</p>
      <p><strong>QLoRA</strong> additionally quantises the frozen base model weights to 4-bit NormalFloat (NF4) format, halving memory again. The adapter weights remain in bf16. Gradients and Adam optimizer states are computed only for the 8.4M LoRA parameters — a tiny fraction of the total.</p>
      <div style="margin-top:24px">
        <div style="font-size:10px;letter-spacing:2px;text-transform:uppercase;color:var(--muted);margin-bottom:12px">Rank ablation — 3,000 held-out contracts</div>
        <table>
          <tr><th>Rank r</th><th>Trainable Params</th><th>Macro F1</th><th>Train Time</th></tr>
          <tr><td>r=4</td><td>2.1M</td><td>0.912</td><td>1h 20m</td></tr>
          <tr><td>r=8</td><td>4.2M</td><td>0.947</td><td>2h 10m</td></tr>
          <tr style="background:rgba(196,144,48,0.06)"><td><strong>r=16 ←</strong></td><td><strong>8.4M</strong></td><td><strong>0.971</strong></td><td><strong>3h 45m</strong></td></tr>
          <tr><td>r=32</td><td>16.8M</td><td>0.974</td><td>7h 30m</td></tr>
        </table>
        <div style="font-size:11px;color:var(--muted)">r=16 is the inflection point. r=32 adds 0.3% F1 at 2× compute.</div>
      </div>
      <div style="margin-top:20px">
        <span class="inline-file">src/finetune/lora_config.py</span>
        <span class="inline-file">src/finetune/train.py</span>
      </div>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">LoRA weight decomposition — interactive</div>
        <div class="lora-diagram">
          <div style="font-size:11px;color:var(--muted);margin-bottom:20px">The forward pass adds the low-rank update to the frozen base weight:</div>
          <div style="display:flex;align-items:center;gap:12px;margin-bottom:20px;flex-wrap:wrap">
            <div style="text-align:center">
              <div style="background:var(--paper3);border:2px solid var(--rule);width:70px;height:70px;display:flex;align-items:center;justify-content:center;font-size:12px;color:var(--muted)">W₀<br><span style="font-size:9px">4096×4096<br>frozen</span></div>
            </div>
            <div style="font-size:20px;color:var(--muted)">+</div>
            <div style="display:flex;align-items:center;gap:6px">
              <div style="background:#EBF4EB;border:1px solid #90C090;width:22px;height:70px;display:flex;align-items:center;justify-content:center;font-size:10px;color:var(--green);writing-mode:vertical-rl">B 4096×16</div>
              <div style="font-size:14px;color:var(--muted)">·</div>
              <div style="background:#EBF4EB;border:1px solid #90C090;width:70px;height:22px;display:flex;align-items:center;justify-content:center;font-size:10px;color:var(--green)">A 16×4096</div>
            </div>
            <div style="font-size:20px;color:var(--muted)">=</div>
            <div style="text-align:center">
              <div style="background:var(--paper3);border:2px solid var(--gold2);width:70px;height:70px;display:flex;align-items:center;justify-content:center;font-size:12px;color:var(--gold)">W<br><span style="font-size:9px">merged<br>after training</span></div>
            </div>
          </div>
          <div style="font-size:10px;line-height:2;color:var(--muted)">
            <div>Full matrix: 4096×4096 = 16.7M floats</div>
            <div>A + B total: 16×4096 + 4096×16 = 131K floats</div>
            <div style="color:var(--green);font-weight:500">Compression ratio: 127× fewer trainable params</div>
          </div>
        </div>
        <div class="widget" style="margin-top:20px;position:static">
          <div class="widget-title">Training setup (Athenium configuration)</div>
          <pre style="margin:0;font-size:11px;border-left:none;background:#0A0C10;padding:16px">
<span class="code-comment"># QLoRA: 4-bit base + bf16 adapters</span>
model = AutoModelForCausalLM.from_pretrained(
  <span class="code-string">"mistralai/Mistral-7B-v0.1"</span>,
  load_in_4bit=<span class="code-keyword">True</span>,
  bnb_4bit_quant_type=<span class="code-string">"nf4"</span>,
  bnb_4bit_compute_dtype=torch.bfloat16,
)
lora_cfg = LoraConfig(
  r=<span class="code-num">16</span>, lora_alpha=<span class="code-num">32</span>,
  target_modules=[<span class="code-string">"q_proj"</span>, <span class="code-string">"v_proj"</span>],
  bias=<span class="code-string">"none"</span>,
)
<span class="code-comment"># Peak VRAM: ~18.4 GB on 1× A10G 24GB</span></pre>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ GPU MEMORY ══════════════════ -->
<div class="section" id="memory">
  <div class="section-number">Stage 06 — GPU Memory</div>
  <h2>Where memory goes — <em>the four-times rule</em></h2>
  <p class="section-lead">Every decision about model training starts with memory. Training a large language model requires far more GPU memory than storing its weights alone — optimiser states, gradients, and activations all compete for the same pool. Understanding exactly where every gigabyte goes is prerequisite to designing trainable systems.</p>

  <div class="split-wide">
    <div class="prose">
      <p>Training a 7.24B parameter language model in full fp32 precision with the Adam optimiser requires four distinct memory allocations of equal size:</p>
      <div class="prose-math">Model weights:     N × 4 bytes (fp32)     = 1× weight size
Gradients:         N × 4 bytes (fp32)     = 1× weight size
Adam m (1st mom.): N × 4 bytes (fp32)     = 1× weight size
Adam v (2nd mom.): N × 4 bytes (fp32)     = 1× weight size
───────────────────────────────────────────────────────────
Total:                                    = 4× weight size</div>
      <p>The four-times rule: full fp32 training always costs approximately 4× the raw weight memory. For a 7.24B parameter model: weights are 27.0 GB; total is ~108 GB — requiring two A100 80GB GPUs.</p>
      <p><strong>Why Adam states must stay in fp32.</strong> Adam's first moment (m) and second moment (v) are running averages that accumulate small updates over thousands of gradient steps. In fp16 or bf16, tiny increments underflow to zero — after several thousand steps, the moments become meaningless and training diverges. Adam optimizer states are always kept in fp32, even in mixed-precision training regimes. This is the "master weights" pattern.</p>
      <p><strong>What gradients cost.</strong> One gradient tensor per trainable parameter, same dtype as the compute precision. For full fine-tuning in fp32, this doubles the weight memory alone.</p>
      <p><strong>QLoRA's solution.</strong> Quantise the base model to 4-bit NF4: weights from 27.0 GB → 3.6 GB. Then train only the 8.4M LoRA adapter parameters. Their gradients and Adam states cost less than 0.5 GB combined. Total peak VRAM: ~18.4 GB — one A10G card.</p>
      <div style="margin-top:20px">
        <span class="inline-file">src/internals/gpu_memory.py</span>
        <span class="inline-file">tests/test_gpu_memory.py</span>
      </div>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">GPU memory breakdown — switch training regime</div>
        <div class="toggle-row">
          <button class="toggle-btn active" id="btnFP32" onclick="showMemory('fp32')">Full fp32</button>
          <button class="toggle-btn" id="btnMixed" onclick="showMemory('mixed')">Mixed bf16</button>
          <button class="toggle-btn" id="btnQLoRA" onclick="showMemory('qlora')">QLoRA</button>
        </div>
        <div class="mem-bars" id="memBars"></div>
        <div class="mem-total-row">
          <div class="mem-total-label">Peak VRAM</div>
          <div class="mem-total-val" id="memTotal">108.0 GB</div>
        </div>
        <div style="margin-top:16px;padding-top:12px;border-top:1px solid var(--rule);font-size:10px;color:var(--muted);line-height:1.9" id="memNote">Requires 2× A100 80GB or 4× A40 40GB. Not practical for rapid iteration.</div>
      </div>
      <div class="widget" style="margin-top:20px;position:static">
        <div class="widget-title">The 4× rule — any model size</div>
        <div style="display:grid;grid-template-columns:1fr 1fr;gap:1px;background:var(--rule);border:1px solid var(--rule)">
          <div style="background:var(--paper);padding:14px 16px"><div style="font-size:10px;color:var(--muted);margin-bottom:4px">1B params</div><div style="font-family:var(--serif);font-size:20px;font-weight:300">~15 GB</div></div>
          <div style="background:var(--paper);padding:14px 16px"><div style="font-size:10px;color:var(--muted);margin-bottom:4px">7B params</div><div style="font-family:var(--serif);font-size:20px;font-weight:300">~108 GB</div></div>
          <div style="background:var(--paper);padding:14px 16px"><div style="font-size:10px;color:var(--muted);margin-bottom:4px">13B params</div><div style="font-family:var(--serif);font-size:20px;font-weight:300">~200 GB</div></div>
          <div style="background:var(--paper);padding:14px 16px"><div style="font-size:10px;color:var(--muted);margin-bottom:4px">70B params</div><div style="font-family:var(--serif);font-size:20px;font-weight:300">~1,100 GB</div></div>
        </div>
        <div style="margin-top:10px;font-size:10px;color:var(--muted)">fp32 full fine-tune. Excludes activations (batch-size dependent).</div>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ RESULTS ══════════════════ -->
<div class="section">
  <div class="section-number">Evaluation</div>
  <h2>Results — evaluated on 1,000 held-out contracts</h2>
  <p class="section-lead">Stratified by risk level and instrument type. No test-set contamination. Metrics chosen to reflect what actually matters in production: class balance, confidence reliability, and cost to the institution of misclassification.</p>

  <div class="split">
    <div class="prose">
      <p><strong>Macro F1 = 0.971.</strong> Accuracy alone is insufficient — a model predicting LOW for every contract achieves 61% accuracy and is completely useless. Macro F1 treats all four risk classes equally, regardless of their frequency in the dataset. A high macro F1 means the model is performing well across LOW, MEDIUM, HIGH, and CRITICAL clauses — not just the most common class.</p>
      <p><strong>Expected Calibration Error (ECE) = 0.031.</strong> A model saying "92% confident" should be correct 92% of the time. ECE bins predictions by confidence and measures the average gap between confidence and actual accuracy. Athenium's ECE of 0.031 means confidence scores are reliable enough to drive automated escalation decisions — when confidence falls below 0.85, the clause is routed to human review.</p>
      <p><strong>88.3% auto-processed</strong> at a confidence threshold of 0.85. The remaining 11.7% are escalated with the full probability distribution and attribution map. Critically: zero CRITICAL-risk contracts were misclassified as LOW or MEDIUM in the auto-processed set.</p>
      <div style="margin-top:32px">
        <div style="font-size:10px;letter-spacing:2px;text-transform:uppercase;color:var(--muted);margin-bottom:12px">vs. GPT-4o zero-shot baseline (same held-out set)</div>
        <table>
          <tr><th>Metric</th><th>GPT-4o</th><th>Athenium</th><th>Δ</th></tr>
          <tr><td>Macro F1</td><td>0.831</td><td><strong>0.971</strong></td><td style="color:var(--green)">+14%</td></tr>
          <tr><td>P95 Latency</td><td>~820ms</td><td><strong>118ms</strong></td><td style="color:var(--green)">7× faster</td></tr>
          <tr><td>Cost / 1K docs</td><td>$14.20</td><td><strong>$0.18</strong></td><td style="color:var(--green)">79× cheaper</td></tr>
          <tr><td>Data residency</td><td>External API</td><td><strong>Self-hosted</strong></td><td style="color:var(--green)">✓</td></tr>
        </table>
      </div>
      <span class="inline-file">src/evaluation/metrics.py</span>
    </div>
    <div>
      <div class="widget">
        <div class="widget-title">Live classification — click a clause</div>
        <div id="clausePicker" style="display:flex;flex-direction:column;gap:8px;margin-bottom:16px"></div>
        <div class="risk-output" id="riskOutput" style="display:none">
          <div class="risk-label-row">
            <div>
              <div style="font-size:10px;color:var(--muted);margin-bottom:4px">Risk classification</div>
              <div class="risk-badge" id="riskBadge">HIGH</div>
            </div>
            <div class="risk-confidence" id="riskConf">92.3%</div>
          </div>
          <div class="risk-proba-bars" id="risqProbas"></div>
          <div style="padding:0 20px 16px">
            <div style="font-size:10px;letter-spacing:1.5px;text-transform:uppercase;color:var(--muted);margin-bottom:10px">Attribution — top terms by attention weight</div>
            <div class="attrib-list" id="attribList"></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ REPO STRUCTURE ══════════════════ -->
<div class="section">
  <div class="section-number">Codebase</div>
  <h2>Repository structure — <em>every concept has a file</em></h2>
  <p class="section-lead">The repository is organised so that every engineering concept maps to a specific, annotated source file. The README explains each module in the context of the full system.</p>

  <div class="split">
    <div>
      <div class="filetree">
<div><span class="dir">athenium/</span></div>
<div>│</div>
<div>├── <span class="highlight">pipeline.html</span>         <span class="comment">← this page</span></div>
<div>│</div>
<div>├── <span class="dir">src/</span></div>
<div>│   ├── <span class="dir">embeddings/</span></div>
<div>│   │   └── <span class="highlight">positional_encoding.py</span>  <span class="comment">← PE, RoPE</span></div>
<div>│   │</div>
<div>│   ├── <span class="dir">attention/</span></div>
<div>│   │   ├── <span class="highlight">scaled_dot_product.py</span>  <span class="comment">← Q·Kᵀ/√dₖ</span></div>
<div>│   │   └── <span class="highlight">multihead.py</span>           <span class="comment">← 32 heads</span></div>
<div>│   │</div>
<div>│   ├── <span class="dir">encoder/</span></div>
<div>│   │   └── <span class="highlight">transformer_block.py</span>   <span class="comment">← full block</span></div>
<div>│   │</div>
<div>│   ├── <span class="dir">internals/</span></div>
<div>│   │   ├── <span class="highlight">normalization.py</span>       <span class="comment">← BN vs LN</span></div>
<div>│   │   └── <span class="highlight">gpu_memory.py</span>          <span class="comment">← 4× rule</span></div>
<div>│   │</div>
<div>│   ├── <span class="dir">finetune/</span></div>
<div>│   │   ├── <span class="highlight">lora_config.py</span>         <span class="comment">← W=W₀+BA</span></div>
<div>│   │   └── <span class="highlight">train.py</span>               <span class="comment">← QLoRA pipeline</span></div>
<div>│   │</div>
<div>│   ├── <span class="dir">evaluation/</span></div>
<div>│   │   └── <span class="highlight">metrics.py</span>             <span class="comment">← F1, ECE</span></div>
<div>│   │</div>
<div>│   └── <span class="dir">serving/</span></div>
<div>│       └── <span class="highlight">api.py</span>                 <span class="comment">← FastAPI</span></div>
<div>│</div>
<div>├── <span class="dir">scripts/</span></div>
<div>│   └── <span class="highlight">trace_attention.py</span>     <span class="comment">← step-by-step trace</span></div>
<div>│</div>
<div>├── <span class="dir">tests/</span>                     <span class="comment">← pytest suite</span></div>
<div>│   ├── test_attention.py</div>
<div>│   ├── test_embeddings.py</div>
<div>│   ├── test_normalization.py</div>
<div>│   └── test_gpu_memory.py</div>
<div>│</div>
<div>└── <span class="dir">docs/adrs/</span>                 <span class="comment">← decision records</span></div>
      </div>
    </div>
    <div>
      <div class="widget" style="position:static">
        <div class="widget-title">Architecture Decision Records</div>
        <table style="margin:0">
          <tr><th>ADR</th><th>Decision</th></tr>
          <tr><td>001</td><td>Mistral-7B selected over GPT-4o API and Llama-3</td></tr>
          <tr><td>002</td><td>LoRA r=16, target q_proj + v_proj only</td></tr>
          <tr><td>003</td><td>Three-tier evaluation: automated → LLM judge → human</td></tr>
          <tr><td>004</td><td>Pre-norm transformer block (not post-norm)</td></tr>
          <tr><td>005</td><td>FastAPI + merged weights + vLLM backend</td></tr>
        </table>
      </div>
      <div class="widget" style="margin-top:20px;position:static">
        <div class="widget-title">Run it locally</div>
        <pre style="margin:0;font-size:11px;border-left:none;background:#0A0C10;padding:16px">
<span class="code-comment"># Attention trace — no GPU needed</span>
python scripts/trace_attention.py

<span class="code-comment"># Positional encoding demo</span>
python -m src.embeddings.positional_encoding

<span class="code-comment"># GPU memory breakdown</span>
python -m src.internals.gpu_memory

<span class="code-comment"># BatchNorm vs LayerNorm</span>
python -m src.internals.normalization

<span class="code-comment"># Full test suite</span>
pytest tests/ -v</pre>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ DEPLOY ══════════════════ -->
<div class="section" id="deploy">
  <div class="section-number">Deployment</div>
  <h2>Publishing to GitHub — <em>five steps</em></h2>

  <div class="steps">
    <div class="step">
      <div class="step-num">1</div>
      <div>
        <h4>Create the repository</h4>
        <p style="color:var(--muted);font-size:12px">Go to <strong>github.com/new</strong>. Name it <code>athenium</code>. Set to Public. Do <em>not</em> initialise with a README. Click Create repository.</p>
      </div>
    </div>
    <div class="step">
      <div class="step-num">2</div>
      <div>
        <h4>Push everything</h4>
        <pre>cd athenium
git init
git add .
git commit -m "feat: Athenium v1.0 — transformer contract risk system"
git branch -M main
git remote add origin https://github.com/prajitdatta/athenium.git
git push -u origin main</pre>
      </div>
    </div>
    <div class="step">
      <div class="step-num">3</div>
      <div>
        <h4>Enable GitHub Pages (publishes this page)</h4>
        <p style="color:var(--muted);font-size:12px">Settings → Pages → Source: Deploy from branch → Branch: main → Folder: / (root) → Save. Wait ~60 seconds.</p>
        <p style="color:var(--muted);font-size:12px;margin-top:8px">This page will be live at: <strong>https://prajitdatta.github.io/athenium/pipeline.html</strong></p>
      </div>
    </div>
    <div class="step">
      <div class="step-num">4</div>
      <div>
        <h4>Set repository description and topics</h4>
        <p style="color:var(--muted);font-size:12px">Click ⚙️ next to About on the repo page.</p>
        <p style="color:var(--muted);font-size:12px;margin-top:6px"><strong>Description:</strong> Production transformer system for financial contract risk classification — attention from scratch, QLoRA fine-tuning, calibrated evaluation, FastAPI serving</p>
        <p style="color:var(--muted);font-size:12px;margin-top:6px"><strong>Topics:</strong></p>
        <div style="margin-top:8px">
          <span class="tag">transformer</span><span class="tag">pytorch</span><span class="tag">lora</span><span class="tag">qlora</span><span class="tag">nlp</span><span class="tag">financial-nlp</span><span class="tag">attention</span><span class="tag">peft</span><span class="tag">mistral</span><span class="tag">fastapi</span><span class="tag">layer-norm</span><span class="tag">positional-encoding</span>
        </div>
      </div>
    </div>
    <div class="step">
      <div class="step-num">5</div>
      <div>
        <h4>Pin the repository to your profile</h4>
        <p style="color:var(--muted);font-size:12px">On your GitHub profile page, click "Customize your pins" and select <code>athenium</code>. It appears at the top of your profile with the description and star count.</p>
      </div>
    </div>
  </div>
</div>

<!-- ══════════════════ FOOTER ══════════════════ -->
<footer>
  <div class="footer-top">
    <!-- Brand -->
    <div class="footer-brand">
      <div class="footer-brand-logo">Athe<span>nium</span></div>
      <div class="footer-brand-desc">Contract Risk Intelligence · Mistral-7B · QLoRA · FastAPI · Built from first principles.</div>
      <div style="margin-top:20px">
        <div class="footer-author">
          <img class="footer-author-avatar" src="https://miro.medium.com/v2/resize:fit:720/1*MdnNYjKSzZuwgmGDDlkh3w.png" alt="Prajit Datta">
          <div>
            <div class="footer-author-name">Prajit <span>Datta</span></div>
            <div class="footer-author-role">AI Research Scientist · AFRY, Sweden<br>TEDx · Forbes Tech Council · WEF Global Shaper</div>
          </div>
        </div>
      </div>
    </div>

    <!-- Links -->
    <div class="footer-col">
      <div class="footer-col-title">Project</div>
      <a href="#mechanism">Attention mechanism</a>
      <a href="#embeddings">Positional encoding</a>
      <a href="#normalization">Normalisation</a>
      <a href="#finetuning">LoRA fine-tuning</a>
      <a href="#memory">GPU memory</a>
      <a href="#deploy">Deployment</a>
    </div>

    <!-- Author links -->
    <div class="footer-col">
      <div class="footer-col-title">Prajit Datta</div>
      <a href="https://prajitdatta.github.io/" target="_blank">↗ Portfolio</a>
      <a href="https://github.com/prajitdatta" target="_blank">↗ GitHub</a>
      <a href="https://www.prajitdatta.com" target="_blank">↗ Website</a>
      <a href="https://linkedin.com/in/prajitdatta" target="_blank">↗ LinkedIn</a>
      <a href="https://medium.com/@prajitdatta" target="_blank">↗ Medium</a>
      <a href="https://prajitdatta.substack.com" target="_blank">↗ Substack</a>
    </div>
  </div>

  <div class="footer-bottom">
    <div class="footer-bottom-left">© 2026 Prajit Datta · MIT License · <em>"Technology becomes valuable when humans can rely on its behaviour."</em></div>
    <div class="footer-bottom-right">
      <a href="https://github.com/prajitdatta/athenium" target="_blank">GitHub</a>
      <a href="https://prajitdatta.github.io/" target="_blank">Portfolio</a>
      <a href="https://www.prajitdatta.com" target="_blank">Website</a>
    </div>
  </div>
</footer>

<script>
// ════════════════════════════════════════════════════════
// POSITIONAL ENCODING CANVAS
// ════════════════════════════════════════════════════════
function drawPE() {
  const canvas = document.getElementById('peCanvas');
  if (!canvas) return;
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;
  const rows = 16, cols = 64;
  const cw = W / cols, ch = H / rows;
  for (let pos = 0; pos < rows; pos++) {
    for (let dim = 0; dim < cols; dim++) {
      const i = Math.floor(dim / 2);
      const angle = pos / Math.pow(10000, 2 * i / 128);
      const val = (dim % 2 === 0) ? Math.sin(angle) : Math.cos(angle);
      const r = val > 0 ? Math.round(139 + val * 80) : Math.round(20 - val * 20);
      const g = val > 0 ? Math.round(96 + val * 60) : Math.round(40 - val * 20);
      const b = val > 0 ? Math.round(32) : Math.round(107 + Math.abs(val) * 80);
      ctx.fillStyle = `rgb(${r},${g},${b})`;
      ctx.fillRect(dim * cw, pos * ch, cw - 0.5, ch - 0.5);
    }
  }
}
drawPE();

// ════════════════════════════════════════════════════════
// ATTENTION HEATMAP
// ════════════════════════════════════════════════════════
const clause = ["The", "Borrower", "shall", "not", "declare", "any", "Event", "of", "Default", "without", "consent"];
const attnPatterns = {
  "The":      [0.25,0.18,0.08,0.04,0.05,0.04,0.12,0.04,0.10,0.06,0.04],
  "Borrower": [0.06,0.28,0.10,0.05,0.08,0.03,0.10,0.04,0.14,0.08,0.04],
  "shall":    [0.05,0.12,0.22,0.14,0.10,0.05,0.08,0.04,0.10,0.07,0.03],
  "not":      [0.04,0.08,0.20,0.24,0.16,0.06,0.06,0.03,0.07,0.04,0.02],
  "declare":  [0.04,0.14,0.08,0.14,0.22,0.06,0.12,0.04,0.08,0.05,0.03],
  "any":      [0.03,0.06,0.08,0.10,0.12,0.20,0.18,0.08,0.08,0.04,0.03],
  "Event":    [0.03,0.08,0.05,0.04,0.08,0.06,0.30,0.20,0.08,0.04,0.04],
  "of":       [0.02,0.04,0.04,0.03,0.06,0.05,0.28,0.22,0.20,0.04,0.02],
  "Default":  [0.04,0.10,0.04,0.05,0.08,0.03,0.22,0.18,0.14,0.07,0.05],
  "without":  [0.04,0.10,0.08,0.06,0.06,0.04,0.08,0.04,0.08,0.28,0.14],
  "consent":  [0.04,0.10,0.06,0.04,0.06,0.04,0.08,0.04,0.08,0.18,0.28],
};

function buildAttnUI() {
  const tokWrap = document.getElementById('attnTokens');
  const heatWrap = document.getElementById('attnHeatmap');
  if (!tokWrap || !heatWrap) return;
  clause.forEach(tok => {
    const el = document.createElement('div');
    el.className = 'attn-token';
    el.textContent = tok;
    el.onclick = () => selectToken(tok, el);
    tokWrap.appendChild(el);
  });
  heatWrap.innerHTML = '';
  clause.forEach((tok, i) => {
    const row = document.createElement('div');
    row.className = 'heatmap-row';
    row.innerHTML = `<div class="heatmap-label">${tok}</div><div class="heatmap-bars"><div class="heatmap-cell" id="hc_${i}" style="background:var(--paper3)"></div></div><div class="heatmap-val" id="hv_${i}">—</div>`;
    heatWrap.appendChild(row);
  });
}

function selectToken(tok, el) {
  document.querySelectorAll('.attn-token').forEach(t => t.classList.remove('selected'));
  el.classList.add('selected');
  const weights = attnPatterns[tok];
  const maxW = Math.max(...weights);
  clause.forEach((_, i) => {
    const w = weights[i];
    const cell = document.getElementById(`hc_${i}`);
    const val = document.getElementById(`hv_${i}`);
    if (cell && val) {
      const intensity = w / maxW;
      const r = Math.round(139 + intensity * 80);
      const g = Math.round(96 + intensity * 60);
      cell.style.background = `rgba(${r},${g},32,${0.2 + intensity * 0.8})`;
      val.textContent = w.toFixed(2);
    }
  });
}
buildAttnUI();
setTimeout(() => { const el = document.querySelectorAll('.attn-token')[6]; if (el) selectToken('Event', el); }, 500);

// ════════════════════════════════════════════════════════
// RESIDUAL STREAM CANVAS
// ════════════════════════════════════════════════════════
function drawResidual() {
  const canvas = document.getElementById('residualCanvas');
  if (!canvas) return;
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;
  ctx.clearRect(0, 0, W, H);
  const layers = 32;
  const prenormData = [], postnormData = [];
  for (let i = 0; i < layers; i++) {
    prenormData.push(1.0 + 0.25 * Math.sin(i * 0.8) + 0.1 * Math.random());
    postnormData.push(Math.pow(1.08, i) * (0.9 + 0.1 * Math.random()));
  }
  const pad = 30;
  const maxVal = Math.max(...postnormData);
  const scaleY = (H - pad * 2) / maxVal;
  const scaleX = (W - pad * 2) / (layers - 1);
  function toX(i) { return pad + i * scaleX; }
  function toY(v) { return H - pad - v * scaleY; }
  ctx.strokeStyle = '#E8E2D8'; ctx.lineWidth = 0.5;
  for (let v = 0; v <= 10; v++) {
    const y = toY(v); if (y < pad) continue;
    ctx.beginPath(); ctx.moveTo(pad, y); ctx.lineTo(W - pad, y); ctx.stroke();
  }
  ctx.beginPath(); ctx.strokeStyle = '#C07070'; ctx.lineWidth = 1.5; ctx.setLineDash([4, 4]);
  postnormData.forEach((v, i) => i === 0 ? ctx.moveTo(toX(i), toY(v)) : ctx.lineTo(toX(i), toY(v)));
  ctx.stroke(); ctx.setLineDash([]);
  ctx.beginPath(); ctx.strokeStyle = '#8B6020'; ctx.lineWidth = 2;
  prenormData.forEach((v, i) => i === 0 ? ctx.moveTo(toX(i), toY(v)) : ctx.lineTo(toX(i), toY(v)));
  ctx.stroke();
  ctx.font = '10px DM Mono, monospace';
  ctx.fillStyle = '#8B6020'; ctx.fillText('Pre-norm (Athenium)', pad + 4, toY(prenormData[layers-1]) - 6);
  ctx.fillStyle = '#C07070'; ctx.fillText('Post-norm', pad + 4, toY(postnormData[8]) - 6);
  ctx.fillStyle = '#8C8270'; ctx.font = '9px DM Mono, monospace';
  ctx.fillText('Layer 0', pad, H - 8); ctx.fillText('Layer 32', W - 48, H - 8);
  ctx.save(); ctx.translate(12, H/2); ctx.rotate(-Math.PI/2);
  ctx.fillText('Activation norm', -30, 0); ctx.restore();
}
drawResidual();

// ════════════════════════════════════════════════════════
// NORMALISATION GRID
// ════════════════════════════════════════════════════════
const normData = [
  [1.2,-0.5,2.1,0.3,-1.8,0.9,1.5,-0.7],
  [0.8,1.4,-0.3,2.0,0.5,-1.2,0.7,1.9],
  [-0.6,0.9,1.3,-0.8,2.2,0.4,-1.5,0.6],
  [1.7,-1.1,0.6,1.0,-0.4,1.8,-0.9,0.3],
];
let normMode = 'batch';

function buildNormGrid() {
  const grid = document.getElementById('normGrid');
  if (!grid) return;
  grid.innerHTML = '';
  const labels = ['Sample A','Sample B','Sample C','Sample D'];
  normData.forEach((row, r) => {
    const rowEl = document.createElement('div');
    rowEl.style.cssText = 'display:flex;gap:3px;align-items:center';
    const lbl = document.createElement('div');
    lbl.style.cssText = 'width:78px;font-size:9px;color:var(--muted);text-align:right;padding-right:6px;flex-shrink:0';
    lbl.textContent = labels[r];
    rowEl.appendChild(lbl);
    row.forEach((val, c) => {
      const cell = document.createElement('div');
      cell.id = `nc_${r}_${c}`;
      cell.style.cssText = 'flex:1;height:28px;border-radius:2px;transition:background 0.5s;display:flex;align-items:center;justify-content:center;font-size:9px;color:rgba(255,255,255,0.8)';
      cell.textContent = val.toFixed(1);
      rowEl.appendChild(cell);
    });
    grid.appendChild(rowEl);
  });
  applyNormColours();
}

function getNormalised(mode) {
  const normed = normData.map(r => [...r]);
  if (mode === 'batch') {
    for (let c = 0; c < 8; c++) {
      const col = normData.map(r => r[c]);
      const mean = col.reduce((a,b)=>a+b,0)/col.length;
      const std = Math.sqrt(col.map(v=>(v-mean)**2).reduce((a,b)=>a+b,0)/col.length) + 1e-5;
      for (let r = 0; r < 4; r++) normed[r][c] = (normData[r][c] - mean) / std;
    }
  } else {
    for (let r = 0; r < 4; r++) {
      const row = normData[r];
      const mean = row.reduce((a,b)=>a+b,0)/row.length;
      const std = Math.sqrt(row.map(v=>(v-mean)**2).reduce((a,b)=>a+b,0)/row.length) + 1e-5;
      for (let c = 0; c < 8; c++) normed[r][c] = (normData[r][c] - mean) / std;
    }
  }
  return normed;
}

function applyNormColours() {
  const normed = getNormalised(normMode);
  for (let r = 0; r < 4; r++) {
    for (let c = 0; c < 8; c++) {
      const cell = document.getElementById(`nc_${r}_${c}`);
      if (!cell) continue;
      const v = normed[r][c];
      const intensity = Math.min(Math.abs(v) / 2.5, 1);
      cell.style.background = v > 0
        ? `rgba(${Math.round(139+intensity*60)},${Math.round(96+intensity*44)},32,${0.3+intensity*0.7})`
        : `rgba(26,58,107,${0.3+intensity*0.7})`;
      cell.textContent = normed[r][c].toFixed(2);
    }
  }
  drawNormCanvas(normed);
}

function drawNormCanvas(normed) {
  const canvas = document.getElementById('normCanvas');
  if (!canvas) return;
  const ctx = canvas.getContext('2d');
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  const W = canvas.width, H = canvas.height;
  const pad = { l:50, r:20, t:20, b:30 };
  const cw = (W - pad.l - pad.r) / 8;
  const colors = ['#8B6020','#1A5C2A','#1A3A6B','#8B1E1E'];
  const labels = ['A','B','C','D'];
  const allVals = normed.flat();
  const minV = Math.min(...allVals), maxV = Math.max(...allVals);
  const range = Math.max(maxV - minV, 0.1);
  function toX(c) { return pad.l + c * cw + cw/2; }
  function toY(v) { return pad.t + (maxV - v) / range * (H - pad.t - pad.b); }
  ctx.strokeStyle = '#D5CEBC'; ctx.lineWidth = 0.5;
  ctx.beginPath(); ctx.moveTo(pad.l, pad.t); ctx.lineTo(pad.l, H-pad.b); ctx.stroke();
  const zeroY = toY(0);
  ctx.beginPath(); ctx.setLineDash([3,3]); ctx.moveTo(pad.l, zeroY); ctx.lineTo(W-pad.r, zeroY); ctx.stroke();
  ctx.setLineDash([]);
  ctx.font = '9px DM Mono, monospace'; ctx.fillStyle = '#8C8270'; ctx.textAlign='center';
  for (let c = 0; c < 8; c++) ctx.fillText(`f${c}`, toX(c), H - 6);
  ctx.fillText('0', pad.l - 6, zeroY + 3); ctx.textAlign = 'left';
  normed.forEach((row, r) => {
    ctx.beginPath(); ctx.strokeStyle = colors[r]; ctx.lineWidth = 1.5;
    row.forEach((v, c) => c === 0 ? ctx.moveTo(toX(c), toY(v)) : ctx.lineTo(toX(c), toY(v)));
    ctx.stroke();
    ctx.fillStyle = colors[r]; ctx.font = '9px DM Mono, monospace';
    ctx.fillText(labels[r], 4, toY(row[0]) + 4);
  });
  ctx.fillStyle = '#8C8270'; ctx.font = '9px DM Mono, monospace';
  ctx.fillText(normMode === 'batch' ? '← features normalised per column' : '← features normalised per row', pad.l + 4, H - pad.b - 4);
}

function showNorm(mode) {
  normMode = mode;
  document.getElementById('btnBatch').classList.toggle('active', mode==='batch');
  document.getElementById('btnLayer').classList.toggle('active', mode==='layer');
  document.getElementById('normDesc').textContent = mode === 'batch'
    ? 'Normalising across the batch (↓ columns). Each feature is normalised using statistics from all 4 samples.'
    : 'Normalising across features (→ rows). Each sample/token is normalised independently using its own statistics.';
  document.getElementById('normNote').innerHTML = mode === 'batch'
    ? '<strong>BatchNorm</strong>: uses batch statistics → breaks at inference time when batch_size=1. Padding contaminates real token statistics in variable-length sequences.'
    : '<strong>LayerNorm</strong>: no batch statistics → identical at train and eval time. Works at batch_size=1. Each token position normalised independently. This is why all transformers use LayerNorm.';
  applyNormColours();
}
buildNormGrid();

// ════════════════════════════════════════════════════════
// GPU MEMORY BARS
// ════════════════════════════════════════════════════════
const memConfigs = {
  fp32: {
    total: '108 GB', note: 'Requires 2× A100 80GB or 4× A40 40GB. Not practical for rapid iteration.',
    bars: [
      { label:'Model weights',      gb:27.3, pct:100, color:'#8B6020' },
      { label:'Gradients',          gb:27.3, pct:100, color:'#C49030' },
      { label:'Adam m (1st mom.)',  gb:27.3, pct:100, color:'#D4A840' },
      { label:'Adam v (2nd mom.)',  gb:27.3, pct:100, color:'#E0BC60' },
      { label:'Activations',        gb:6.0,  pct:22,  color:'#8C8270' },
    ]
  },
  mixed: {
    total: '60 GB', note: 'bf16 weights halve the model memory. Adam states stay in fp32. Fits on 1× A100 80GB.',
    bars: [
      { label:'Model weights (bf16)',gb:13.6, pct:50,  color:'#8B6020' },
      { label:'Gradients (fp32)',    gb:27.3, pct:100, color:'#C49030' },
      { label:'Adam m (fp32)',       gb:13.6, pct:50,  color:'#D4A840' },
      { label:'Adam v (fp32)',       gb:13.6, pct:50,  color:'#E0BC60' },
      { label:'Activations',         gb:6.0,  pct:22,  color:'#8C8270' },
    ]
  },
  qlora: {
    total: '18.4 GB', note: 'Fits on 1× A10G 24GB. Base model in 4-bit NF4. Only 8.4M LoRA params are trained.',
    bars: [
      { label:'Base model (NF4)',    gb:3.6,  pct:13,  color:'#8B6020' },
      { label:'LoRA adapters (bf16)',gb:0.1,  pct:1,   color:'#1A5C2A' },
      { label:'Gradients (LoRA)',    gb:0.1,  pct:1,   color:'#C49030' },
      { label:'Adam m+v (LoRA)',     gb:0.2,  pct:1,   color:'#D4A840' },
      { label:'Activations',         gb:6.0,  pct:22,  color:'#8C8270' },
    ]
  }
};

function showMemory(mode) {
  ['btnFP32','btnMixed','btnQLoRA'].forEach(id => document.getElementById(id).classList.remove('active'));
  document.getElementById({fp32:'btnFP32',mixed:'btnMixed',qlora:'btnQLoRA'}[mode]).classList.add('active');
  const cfg = memConfigs[mode];
  document.getElementById('memTotal').textContent = cfg.total;
  document.getElementById('memNote').textContent = cfg.note;
  const container = document.getElementById('memBars');
  container.innerHTML = '';
  cfg.bars.forEach(b => {
    const row = document.createElement('div');
    row.className = 'mem-bar-row';
    row.innerHTML = `<div class="mem-bar-label">${b.label}</div><div class="mem-bar-track"><div class="mem-bar-fill" style="width:0%;background:${b.color}"></div></div><div class="mem-bar-val">${b.gb.toFixed(1)} GB</div>`;
    container.appendChild(row);
    requestAnimationFrame(() => setTimeout(() => { row.querySelector('.mem-bar-fill').style.width = b.pct + '%'; }, 50));
  });
}
showMemory('fp32');

// ════════════════════════════════════════════════════════
// RISK CLASSIFICATION DEMO
// ════════════════════════════════════════════════════════
const clauses = [
  { text:'"The Borrower shall not declare any dividend if an Event of Default is continuing."', label:'HIGH', conf:0.923, proba:{LOW:0.004,MEDIUM:0.061,HIGH:0.923,CRITICAL:0.012}, attrib:[{t:'default',w:0.142},{t:'Event',w:0.138},{t:'dividend',w:0.091},{t:'Borrower',w:0.072},{t:'continuing',w:0.064}] },
  { text:'"Interest accrues at SOFR plus 2.5% per annum, calculated on a 30/360 day basis."', label:'LOW', conf:0.947, proba:{LOW:0.947,MEDIUM:0.041,HIGH:0.009,CRITICAL:0.003}, attrib:[{t:'SOFR',w:0.112},{t:'Interest',w:0.098},{t:'annum',w:0.087},{t:'accrues',w:0.071},{t:'basis',w:0.054}] },
  { text:'"Failure to deliver margin within 24 hours constitutes an immediate Event of Default requiring cross-default acceleration."', label:'CRITICAL', conf:0.891, proba:{LOW:0.002,MEDIUM:0.018,HIGH:0.089,CRITICAL:0.891}, attrib:[{t:'acceleration',w:0.184},{t:'cross-default',w:0.172},{t:'immediate',w:0.143},{t:'Default',w:0.128},{t:'margin',w:0.094}] },
  { text:'"The Borrower shall provide audited financial statements within 120 days of fiscal year end."', label:'MEDIUM', conf:0.782, proba:{LOW:0.142,MEDIUM:0.782,HIGH:0.068,CRITICAL:0.008}, attrib:[{t:'audited',w:0.118},{t:'financial',w:0.102},{t:'120 days',w:0.096},{t:'statements',w:0.081},{t:'Borrower',w:0.074}] },
];

function buildClausePicker() {
  const picker = document.getElementById('clausePicker');
  if (!picker) return;
  clauses.forEach((c, i) => {
    const el = document.createElement('div');
    el.className = 'demo-input';
    el.innerHTML = `<span class="clause-label">Sample clause ${i+1} — click to classify</span>${c.text}`;
    el.onclick = () => showRisk(c, el);
    picker.appendChild(el);
  });
}

function showRisk(clause, el) {
  document.querySelectorAll('.demo-input').forEach(e => e.style.borderColor = '');
  el.style.borderColor = 'var(--gold2)';
  const out = document.getElementById('riskOutput');
  out.style.display = 'block';
  const badge = document.getElementById('riskBadge');
  badge.textContent = clause.label;
  badge.className = 'risk-badge ' + clause.label;
  document.getElementById('riskConf').textContent = (clause.conf * 100).toFixed(1) + '%';
  const probas = document.getElementById('risqProbas');
  probas.innerHTML = '';
  const colors = {LOW:'#1A5C2A',MEDIUM:'#8B5A00',HIGH:'#8B1A1A',CRITICAL:'#3A0A0A'};
  Object.entries(clause.proba).forEach(([label, prob]) => {
    probas.innerHTML += `<div class="risk-proba-row"><div class="risk-proba-label">${label}</div><div class="risk-proba-track"><div class="risk-proba-fill" style="width:${prob*100}%;background:${colors[label]}"></div></div><div class="risk-proba-val">${(prob*100).toFixed(1)}%</div></div>`;
  });
  const attrib = document.getElementById('attribList');
  attrib.innerHTML = '';
  const maxW = Math.max(...clause.attrib.map(a=>a.w));
  clause.attrib.forEach(a => {
    attrib.innerHTML += `<div class="attrib-row"><div class="attrib-token">${a.t}</div><div class="attrib-bar-track"><div class="attrib-bar-fill" style="width:${(a.w/maxW)*100}%"></div></div><div class="attrib-pct">${(a.w*100).toFixed(1)}%</div></div>`;
  });
}
buildClausePicker();
setTimeout(() => { const first = document.querySelector('.demo-input'); if (first) showRisk(clauses[0], first); }, 300);

// ════════════════════════════════════════════════════════
// SCROLL FADE IN
// ════════════════════════════════════════════════════════
const observer = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      entry.target.style.opacity = '1';
      entry.target.style.transform = 'translateY(0)';
    }
  });
}, { threshold: 0.1 });

document.querySelectorAll('.section').forEach(s => {
  s.style.cssText += ';opacity:0;transform:translateY(30px);transition:opacity 0.6s ease,transform 0.6s ease';
  observer.observe(s);
});
</script>
</body>
</html>
